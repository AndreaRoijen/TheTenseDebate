{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCmVIZo30dMx"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ds0g7mE10dMy"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense,LSTM,Bidirectional,Input,Concatenate,Embedding,Attention,TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "import math\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr,spearmanr\n",
    "nan = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqa1-3iZ0dM0"
   },
   "source": [
    "## Initialisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dtLi4shd0dM1"
   },
   "outputs": [],
   "source": [
    "# TYPE/TOKEN FREQUENCY\n",
    "use_token_freq = False # True for token frequency model, false for type frequency model\n",
    "square_root_normalization = False # True for token frequency model, false for type frequency model\n",
    "\n",
    "# HYPERPARAMETER SETTINGS\n",
    "latent_dim = 100\n",
    "embedding_dim = 300\n",
    "batch_size = 32\n",
    "patience = 15\n",
    "dropout = 0.3\n",
    "lr = 0.001\n",
    "\n",
    "# FIXED SETTINGS after Experiment 1\n",
    "track = 'val_loss' #'val_loss' for validation loss / 'loss' for training loss tracking with early stopping criterion\n",
    "excl_extra_verbs = False\n",
    "remove_doubles = False\n",
    "SEED = 4 # Same seed for same data split across different expeirments\n",
    "val_split = 0.8 # Do not change, because we use 5 different data splits based on an 80/20 split\n",
    "beam_width = 12\n",
    "max_epochs = 400 # Early stopping is used so max is never reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose one: experiment 1 / experiment 2 / both\n",
    "exp1 = True\n",
    "exp2 = False\n",
    "exp_1_2 = False\n",
    "\n",
    "# Run only default data fold 5 times (as done for experiment 1) -- referred to as split 1 in the report\n",
    "if exp1:\n",
    "    number_of_initialisations = 5\n",
    "    fold = 'DEFAULT'\n",
    "\n",
    "# Run the 4 other data splits 5 times, in addition to the one of experiment 1 (as done for experiment 2)\n",
    "if exp2:\n",
    "    number_of_initialisations = 20\n",
    "    fold = 'first20'\n",
    "\n",
    "# Run all data splits 5 times\n",
    "if exp_1_2:\n",
    "    number_of_initialisations = 25\n",
    "    fold = 'first20'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions needed during the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that:\n",
    "- \"split 1\" in the thesis report refers to the \"DEFAULT\" split in this code! This datasplit used for Experiment 1.\n",
    "- \"val\"/\"validation\" here means \"development set\" that is mentioned in the thesis report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and split data (depending on experiment settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_data(square_root_normalization, SEED, val_split, fold, excl_extra_verbs, remove_doubles):\n",
    "    # CELEX verb set based on past tense form frequency => 1\n",
    "    with open(\"my_dataset.txt\", encoding=\"utf8\") as dataset:\n",
    "        verblist = [line.strip().split(\"\\t\") for line in dataset][1:]\n",
    "    # A&H data set from K&C's repository, CELEX verb set based on lemma frequency > 10\n",
    "    with open(\"KCamerican.txt\", encoding=\"utf8\") as KC: #\n",
    "        KClist = [line.strip().split(\"\\t\") for line in KC][1:]\n",
    "    \n",
    "    # Initialize lists\n",
    "    freq, pairs, written_pairs, label = [], [], [], []\n",
    "    \n",
    "    # Find intersection of datasets based on specific columns\n",
    "    for lemma in verblist:\n",
    "        for KClemma in KClist:\n",
    "            if KClemma[3] == lemma[2] and KClemma[4] == lemma[3]:\n",
    "                pairs.append([KClemma[0], KClemma[1]])          # IPA verb pairs (present/past tense) from K&C data\n",
    "                written_pairs.append([lemma[2], lemma[3]])      # Verb pairs in written form\n",
    "                freq.append(int(lemma[0]))                      # Past tense token frequency\n",
    "                label.append(KClemma[-1])                       # Morphological class (regular/irregular) from K&C data\n",
    "    \n",
    "    # Add missing verbs from an external file (verbs that appeared to be omitted from A&H's data)\n",
    "    with open(\"missing.txt\", encoding=\"utf8\") as missing:\n",
    "        additional_verbs = [line.strip().split(\"\\t\") for line in missing]\n",
    "        \n",
    "    for verb in additional_verbs:\n",
    "        pairs.insert(0, [verb[2], verb[3]])                    # Insert at the beginning\n",
    "        written_pairs.insert(0, [verb[0], verb[1]])\n",
    "        freq.insert(0, int(verb[4]))\n",
    "        label.insert(0, verb[5])\n",
    "    \n",
    "    # consolidate identical examples\n",
    "    new_pairs, new_written_pairs, new_freq, new_label = [], [], [], []\n",
    "    \n",
    "    for i, wp in enumerate(written_pairs):\n",
    "        if wp in new_written_pairs:\n",
    "            idx = new_written_pairs.index(wp)\n",
    "            new_freq[idx] += freq[i]  # Sum token frequency for duplicates\n",
    "        else:\n",
    "            new_written_pairs.append(wp)\n",
    "            new_pairs.append(pairs[i])\n",
    "            new_freq.append(freq[i])\n",
    "            new_label.append(label[i])\n",
    "    \n",
    "    # Update variables with cleaned data\n",
    "    written_pairs, pairs, freq, label = new_written_pairs, new_pairs, new_freq, new_label\n",
    "    \n",
    "    # Normalize token frequencies using square root if needed\n",
    "    if square_root_normalization:\n",
    "        freq = [int(math.sqrt(f)) for f in freq]\n",
    "        \n",
    "    # Shuffle data with a fixed seed\n",
    "    for lst in [pairs, written_pairs, freq, label]:\n",
    "        random.Random(SEED).shuffle(lst)\n",
    "\n",
    "    # Split data into train-validation sets\n",
    "    split_idx = int(val_split * len(pairs))\n",
    "    train_pairs, val_pairs = pairs[:split_idx], pairs[split_idx:]\n",
    "    train_wp, val_wp = written_pairs[:split_idx], written_pairs[split_idx:]\n",
    "    train_freq, val_freq = freq[:split_idx], freq[split_idx:]\n",
    "    train_label, val_label = label[:split_idx], label[split_idx:]\n",
    "\n",
    "    # Calculate fold boundaries for splitting the data into five folds (80/20 train-val)\n",
    "    fold_size = len(val_pairs)\n",
    "    fold2, fold3, fold4, fold5 = fold_size, fold_size * 2, fold_size * 3, fold_size * 4\n",
    "\n",
    "    # Create train-validation splits based on the fold selected. If fold == 'DEFAULT' (Experiment 1) then data is split as fold 5 / fifth 20%\n",
    "    if fold == 'first20':\n",
    "        val_pairs, train_pairs = pairs[:fold2], pairs[fold2:]\n",
    "        val_wp, train_wp = written_pairs[:fold2], written_pairs[fold2:]\n",
    "        val_freq, train_freq = freq[:fold2], freq[fold2:]\n",
    "        val_label, train_label = label[:fold2], label[fold2:]\n",
    "        \n",
    "    elif fold == 'second20':\n",
    "        val_pairs, train_pairs = pairs[fold2:fold3], pairs[:fold2] + pairs[fold3:]\n",
    "        val_wp, train_wp = written_pairs[fold2:fold3], written_pairs[:fold2] + written_pairs[fold3:]\n",
    "        val_freq, train_freq = freq[fold2:fold3], freq[:fold2] + freq[fold3:]\n",
    "        val_label, train_label = label[fold2:fold3], label[:fold2] + label[fold3:]\n",
    "        \n",
    "    elif fold == 'third20':\n",
    "        val_pairs, train_pairs = pairs[fold3:fold4], pairs[:fold3] + pairs[fold4:]\n",
    "        val_wp, train_wp = written_pairs[fold3:fold4], written_pairs[:fold3] + written_pairs[fold4:]\n",
    "        val_freq, train_freq = freq[fold3:fold4], freq[:fold3] + freq[fold4:]\n",
    "        val_label, train_label = label[fold3:fold4], label[:fold3] + label[fold4:]\n",
    "        \n",
    "    elif fold == 'fourth20':\n",
    "        val_pairs, train_pairs = pairs[fold4:fold5], pairs[:fold4] + pairs[fold5:]\n",
    "        val_wp, train_wp = written_pairs[fold4:fold5], written_pairs[:fold4] + written_pairs[fold5:]\n",
    "        val_freq, train_freq = freq[fold4:fold5], freq[:fold4] + freq[fold5:]\n",
    "        val_label, train_label = label[fold4:fold5], label[:fold4] + label[fold5:]\n",
    "    \n",
    "    # Experiment 1: \n",
    "    # If desired, remove the added missing verbs\n",
    "    if excl_extra_verbs:\n",
    "        removelist = additional_verbs   # Remove the verbs that were added (seems redundant, but they should not be excluded before shuffling and splitting the data as that would lead to different splits compared to the configuration where we keep these verbs in the data)\n",
    "        for p in removelist:\n",
    "            if p in train_pairs:\n",
    "                n = train_pairs.index(p)\n",
    "                train_pairs.pop(n)\n",
    "                train_wp.pop(n)\n",
    "                train_label.pop(n)\n",
    "                train_freq.pop(n)\n",
    "            elif p in val_pairs:\n",
    "                n = val_pairs.index(p)\n",
    "                val_pairs.pop(n)\n",
    "                val_wp.pop(n)\n",
    "                val_label.pop(n)\n",
    "                val_freq.pop(n)\n",
    "    \n",
    "    # Experiment 1:                \n",
    "    # If desired, remove the second(+) inflections of verbs that have more than one correct inflection\n",
    "    if remove_doubles:\n",
    "        removelist,seen,indices=[],[],[]\n",
    "        for i,p in enumerate(pairs):\n",
    "            if p[0] in seen:\n",
    "                for j,s in enumerate(seen):\n",
    "                    if p[0]==s and p[1]!=pairs[indices[j]][1]:  # present==present and past!=past (can also be with past==past)\n",
    "                        frequency = freq[i]\n",
    "                        prev_seen_freq = freq[indices[j]]\n",
    "                        if frequency >= prev_seen_freq:     # Keep the most frequent example\n",
    "                            removelist.append(pairs[indices[j]])\n",
    "                        else:\n",
    "                            removelist.append(p)\n",
    "            seen.append(p[0])\n",
    "            indices.append(i)\n",
    "        for p in removelist:\n",
    "            if p in train_pairs:\n",
    "                n = train_pairs.index(p)\n",
    "                train_pairs.pop(n)\n",
    "                train_wp.pop(n)\n",
    "                train_label.pop(n)\n",
    "                train_freq.pop(n)\n",
    "            elif p in val_pairs:\n",
    "                n = val_pairs.index(p)\n",
    "                val_pairs.pop(n)\n",
    "                val_wp.pop(n)\n",
    "                val_label.pop(n)\n",
    "                val_freq.pop(n)\n",
    "    \n",
    "    # Concatenate training and validation sets again\n",
    "    pairs = train_pairs + val_pairs\n",
    "    written_pairs = train_wp + val_wp\n",
    "    label = train_label + val_label\n",
    "    freq = train_freq + val_freq\n",
    "    split = len(train_pairs)\n",
    "    \n",
    "    return pairs, written_pairs, freq, label, split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-process data to encoder-decoder inputs/targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_data(split, pairs, freq, use_token_freq):\n",
    "    # Create input data and target data for the encoder-decoder model\n",
    "    verbs, input_verbs, target_verbs = [],[],[]\n",
    "    new_pairs = []\n",
    "    characters = set() \n",
    "    \n",
    "    for input_verb, target_verb in pairs: \n",
    "        iv,tv = \"\",\"\"                               \n",
    "        \n",
    "        # Process input verbs (present tense forms)\n",
    "        for X,char in enumerate(input_verb):\n",
    "            # Fixing phonetic transcription from the K&C data\n",
    "            if char == '\\x8d': \n",
    "                char = 'A'\n",
    "            iv += char\n",
    "            # Add every character to the character set, in order to create character_indexation later on\n",
    "            characters.add(char)\n",
    "        input_verbs.append(iv)\n",
    "        \n",
    "        # The same for the target verbs (past tense forms) and add start sign '\\t' and end sign '\\n'\n",
    "        target_verb = \"\\t\" + target_verb + \"\\n\"   \n",
    "        for Y,char in enumerate(target_verb):\n",
    "            if char == '\\x8d':\n",
    "                char = 'A'\n",
    "            tv += char\n",
    "            characters.add(char)\n",
    "        new_pairs.append([iv, tv.strip()]) # new_pairs to update to 'pairs' with the fixed 'A' character\n",
    "        target_verbs.append(tv) \n",
    "        \n",
    "    pairs = new_pairs # new_pairs to update to 'pairs' with the fixed 'A' character\n",
    "    \n",
    "    # Create a character index\n",
    "    characters = sorted(list(characters))\n",
    "    char_index = dict([(char, i) for i, char in enumerate(characters)])\n",
    "    \n",
    "    # Number of characters\n",
    "    num_characters = len(characters)\n",
    "\n",
    "    # Compute max word length and number of samples (verb pairs) to determine array sizes below\n",
    "    verbs = input_verbs + target_verbs\n",
    "    max_word_length = max([len(verb) for verb in verbs])\n",
    "    num_samples = len(pairs)\n",
    "    \n",
    "    # Initialize encoder and decoder input/target data\n",
    "    encoder_input_data = np.zeros((num_samples, max_word_length), dtype=\"float32\") # Representing present tense forms\n",
    "    decoder_input_data = np.zeros((num_samples, max_word_length), dtype=\"float32\") # Representing correct past tense forms for teacher forcing during training\n",
    "    decoder_target_data = np.zeros((num_samples, max_word_length), dtype=\"float32\") # Representing correct past tense forms\n",
    "    \n",
    "    # Fill encoder and decoder data\n",
    "    for i, (input_verb, target_verb) in enumerate(zip(input_verbs, target_verbs)):\n",
    "        for j, char in enumerate(input_verb):\n",
    "            encoder_input_data[i, j] =  char_index[char]\n",
    "        for j, char in enumerate(target_verb):\n",
    "            decoder_input_data[i, j] =  char_index[char]\n",
    "            if j > 0:\n",
    "                decoder_target_data[i, j-1] = char_index[char]  # Exclude start character \\t\", the target sequence is one character ahead of the input (i.e. the previous correct character) \n",
    "\n",
    "   # Training encoder input data (eid), decoder input data (did), decoder target data (dtd)\n",
    "    train_eid = encoder_input_data[:split]\n",
    "    train_did = decoder_input_data[:split]\n",
    "    train_dtd = decoder_target_data[:split]\n",
    "    train_target_verbs = target_verbs[:split]\n",
    "\n",
    "    # Validation encoder input data (eid), decoder input data (did), decoder target data (dtd)\n",
    "    val_eid = encoder_input_data[split:]\n",
    "    val_did = decoder_input_data[split:]\n",
    "    val_dtd = decoder_target_data[split:]\n",
    "    val_target_verbs = target_verbs[split:]\n",
    "    val_data = ([val_eid, val_did], val_dtd)\n",
    "\n",
    "    # If desired (Experiment 1 + 2) augment training part with token frequency\n",
    "    if use_token_freq:\n",
    "        num_samples=0\n",
    "        \n",
    "        # First compute total number of examples if using token frequency\n",
    "        for n,__ in enumerate(train_eid):\n",
    "            for f in range(freq[n]):\n",
    "                num_samples+=1  \n",
    "                \n",
    "        # Initialise new enc/dec input/target data with the updated total number of examples\n",
    "        encoder_input_data = np.zeros((num_samples, max_word_length), dtype=\"float32\")\n",
    "        decoder_input_data = np.zeros((num_samples, max_word_length), dtype=\"float32\")\n",
    "        decoder_target_data = np.zeros((num_samples, max_word_length), dtype=\"float32\")\n",
    "                \n",
    "        # Augment training encoder and decoder input/target data by adding N examples, where N is the token frequency of the verb\n",
    "        t=0\n",
    "        for n,x in enumerate(train_eid):\n",
    "            for f in range(freq[n]):\n",
    "                encoder_input_data[t] = train_eid[n]\n",
    "                decoder_input_data[t] = train_did[n]\n",
    "                decoder_target_data[t] = train_dtd[n]\n",
    "                t+=1  \n",
    "        \n",
    "    else:\n",
    "        encoder_input_data=train_eid\n",
    "        decoder_input_data=train_did\n",
    "        decoder_target_data=train_dtd\n",
    "\n",
    "    return encoder_input_data, train_eid, train_target_verbs, decoder_input_data, decoder_target_data, val_eid, val_target_verbs, char_index, max_word_length, num_characters, val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Wug data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_wug_data(char_index,max_word_length):\n",
    "    wugs,wugs_reg,wugs_irreg1,wugs_irreg2=[],[],[],[]\n",
    "    \n",
    "    # Load all wug forms\n",
    "    with open(\"KCamericanwugs.txt\", encoding=\"utf8\") as KCwug_data: # A&H nonce verb present tense in K&C AE phonetic transcriptions\n",
    "        for wug in KCwug_data:\n",
    "            wugs.append(\"\".join(wug.split()))\n",
    "            \n",
    "    with open(\"reg.txt\", encoding=\"utf8\") as KCwug_data: # A&H regular inflection in K&C AE phonetic transcriptions\n",
    "        for wug in KCwug_data:\n",
    "            wugs_reg.append(\"\".join(wug.split()))       \n",
    "            \n",
    "    with open(\"irreg1.txt\", encoding=\"utf8\") as KCwug_data: # A&H irregular inflection in K&C AE phonetic transcriptions\n",
    "        for wug in KCwug_data:\n",
    "            wugs_irreg1.append(\"\".join(wug.split()))    \n",
    "            \n",
    "    with open(\"irreg2.txt\", encoding=\"utf8\") as KCwug_data: # A&H 2nd option irregular inflection in K&C AE phonetic transcriptions\n",
    "        for wug in KCwug_data:\n",
    "            wugs_irreg2.append(\"\".join(wug.split()))\n",
    "    \n",
    "    # Convert the wug verbs into arrays of integers in the same way as the real verb data\n",
    "    wug_input_data = np.zeros((len(wugs), max_word_length), dtype=\"float32\")\n",
    "    \n",
    "    for i, input_nonce in enumerate(wugs):\n",
    "        for t, char in enumerate(input_nonce):\n",
    "            wug_input_data[i, t] =  char_index[char]\n",
    "            \n",
    "    return wugs, wugs_reg ,wugs_irreg1, wugs_irreg2, wug_input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to train and build Encoder-Decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_build_model(num_characters,max_word_length, encoder_input_data, decoder_input_data, decoder_target_data, val_data):\n",
    "    \n",
    "    ## ENCODER ##\n",
    "    encoder_inputs = Input(shape=(max_word_length,))\n",
    "        \n",
    "    # Embedding layer\n",
    "    enc_emb_layer = Embedding(num_characters, embedding_dim, trainable=True)\n",
    "    enc_emb = enc_emb_layer(encoder_inputs)\n",
    "\n",
    "    # BiLSTM layer 1\n",
    "    enc_lstm1 = Bidirectional(LSTM(int(latent_dim/2), return_sequences=True, return_state=True, dropout=dropout))\n",
    "    encoder_output1, f_h, f_c, b_h, b_c = enc_lstm1(enc_emb, training=True)\n",
    "\n",
    "    c1, c2 = Concatenate(), Concatenate()\n",
    "    e_h = c1([f_h, b_h])\n",
    "    e_c = c2([f_c, b_c])\n",
    "    encoder_states1 = [e_h, e_c]\n",
    "        \n",
    "    # BiLSTM layer 2\n",
    "    enc_lstm2 = Bidirectional(LSTM(int(latent_dim/2), return_sequences=True, return_state=True, dropout=dropout))\n",
    "    encoder_output2, f_h2, f_c2, b_h2, b_c2 = enc_lstm2(encoder_output1, training=True)\n",
    "\n",
    "    c3, c4 = Concatenate(), Concatenate()\n",
    "    e_h2 = c3([f_h2, b_h2])\n",
    "    e_c2 = c4([f_c2, b_c2])\n",
    "    encoder_states2 = [e_h2, e_c2]\n",
    "\n",
    "    ## DECODER ##\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    \n",
    "    # Embedding layer\n",
    "    dec_emb_layer = Embedding(num_characters, embedding_dim, trainable=True)\n",
    "    dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "    # LSTM layer 1\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=dropout)\n",
    "    dec_out, d_h, d_c = decoder_lstm(dec_emb, initial_state=encoder_states1, training=True)\n",
    "    decoder_states = [d_h, d_c]\n",
    "    \n",
    "    # LSTM layer 2\n",
    "    decoder_lstm2 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=dropout)\n",
    "    dec_out2, d_h2, d_c2 = decoder_lstm2(dec_out, initial_state=encoder_states2, training=True)\n",
    "    decoder_states2 = [d_h2, d_c2]\n",
    "\n",
    "    ## ATTENTION ##\n",
    "    ED_attention = Attention()\n",
    "    ed_attention = ED_attention([dec_out2, encoder_output2])\n",
    "\n",
    "    decoder_concat_layer = Concatenate(axis=-1)\n",
    "    dense_input = decoder_concat_layer([dec_out2, ed_attention])\n",
    "\n",
    "    ## DENSE ##\n",
    "    decoder_dense = TimeDistributed(Dense(num_characters, activation='softmax')) \n",
    "    decoder_outputs = decoder_dense(dense_input)\n",
    "    \n",
    "    ## COMPILE AND FIT MODEL ##\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)  \n",
    "    model.compile(optimizer='Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor=track, patience=patience, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit([encoder_input_data, decoder_input_data], decoder_target_data, epochs=max_epochs,\n",
    "                        batch_size=batch_size, validation_data=val_data, callbacks=[early_stopping], shuffle=True)\n",
    "    \n",
    "    print(len(history.history['loss']))\n",
    "    \n",
    "    # Return epoch number to which weights are restored (best ones)\n",
    "    n_epochs = len(history.history['loss'])-patience\n",
    "\n",
    "    ## BUILD INFERENCE MODEL ##\n",
    "    \n",
    "    # Encoder\n",
    "    enc_emb = enc_emb_layer(encoder_inputs) # Embedding\n",
    "    encoder_output1, f_h, f_c, b_h, b_c = enc_lstm1(enc_emb, training=False)\n",
    "    e_h = c1([f_h, b_h])\n",
    "    e_c = c2([f_c, b_c])\n",
    "    encoder_states1 = [e_h, e_c] # BiLSTM layer 1\n",
    "    encoder_output2, f_h2, f_c2, b_h2, b_c2 = enc_lstm2(encoder_output1, training=False)\n",
    "    e_h2 = c3([f_h2, b_h2])\n",
    "    e_c2 = c4([f_c2, b_c2])\n",
    "    encoder_states2 = [e_h2, e_c2] # BiLSTM layer 2\n",
    "\n",
    "    encoder_model = Model(encoder_inputs, [encoder_output2] + encoder_states1 + encoder_states2)\n",
    "\n",
    "    # Decoder\n",
    "    e_states1 = [Input(shape=(latent_dim,)), Input(shape=(latent_dim,))]\n",
    "    e_states2 = [Input(shape=(latent_dim,)), Input(shape=(latent_dim,))]\n",
    "    e_output = Input(shape=(max_word_length, latent_dim))\n",
    "    \n",
    "    dec_emb = dec_emb_layer(decoder_inputs) # Embedding\n",
    "    dec_out, d_h, d_c = decoder_lstm(dec_emb, initial_state=e_states1, training=False)\n",
    "    decoder_states = [d_h, d_c] # LSTM layer 1 \n",
    "    dec_out2, d_h2, d_c2 = decoder_lstm2(dec_out, initial_state=e_states2, training=False)\n",
    "    decoder_states2 = [d_h2, d_c2] # LSTM layer 2\n",
    "    \n",
    "    ed_attention = ED_attention([dec_out2, e_output]) # Attention\n",
    "    dense_input = decoder_concat_layer([dec_out2, ed_attention])\n",
    "    decoder_outputs = decoder_dense(dense_input) # Main task\n",
    "\n",
    "    decoder_model = Model([decoder_inputs] + [e_output] + e_states1 + e_states2,\n",
    "                          [decoder_outputs] + decoder_states + decoder_states2)\n",
    "\n",
    "    return encoder_model, decoder_model, n_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to decode sequences from ED output to verbs (character by character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, encoder_model, decoder_model, max_len, char_index):\n",
    "    # Create a reverse character index, to convert predictions to characters\n",
    "    reverse_char_index = dict((i, char) for char, i in char_index.items())\n",
    "\n",
    "    # Encode input sequence (present tense verb form)\n",
    "    encoder_output, h1, c1, h2, c2 = encoder_model.predict(input_seq, verbose=0)\n",
    "    states_value1 = [h1, c1]\n",
    "    states_value2 = [h2, c2]\n",
    "\n",
    "    # Prepare target sequence (predicted past tense verb form)\n",
    "    current_char = np.array([[char_index[\"\\t\"]]])   # Sequence starts with start sign '\\t'\n",
    "    stop_condition = False\n",
    "    decoded_verb = []\n",
    "\n",
    "    # Loop predicting the next character based on the previous prediction\n",
    "    while not stop_condition:\n",
    "        # Decoder prediction\n",
    "        output, h1, c1, h2, c2 = decoder_model.predict([current_char] + [encoder_output] + states_value1 + states_value2, verbose=0)\n",
    "        \n",
    "        # Get index of predicted character\n",
    "        predicted_char_index = np.argmax(output[0, 0])\n",
    "        predicted_char = reverse_char_index[predicted_char_index]\n",
    "        \n",
    "        # Append predicted character\n",
    "        decoded_verb.append(predicted_char)\n",
    "\n",
    "        # Stopping condition is true if case length exceeds max word length or stop sign '\\n' is found\n",
    "        if predicted_char == \"\\n\" or len(decoded_verb) > max_len:\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            # Updating states to predicted character\n",
    "            current_char[0, 0] = predicted_char_index\n",
    "            states_value1 = [h1, c1]\n",
    "            states_value2 = [h2, c2]\n",
    "\n",
    "    # Return the decoded verb\n",
    "    return ''.join(decoded_verb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search algorithm (used in the Wug Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(input_seq, enc_model, dec_model, beam_width, max_len, char_index):\n",
    "    \n",
    "    # Beam search algorithm used for nonce verb (Wug Test) predictions\n",
    "\n",
    "    # Predict encoding of input sequence (present tense verb)\n",
    "    encoded_seq, h1, c1, h2, c2 = enc_model.predict(input_seq,verbose=0)\n",
    "    state1 = [h1, c1] # Encoder BiLSTM layer 1 states\n",
    "    state2 = [h2, c2] # Encoder BiLSTM layer 2 states\n",
    "    \n",
    "    current_char = np.zeros((1, 1))\n",
    "    current_char[0, 0] = char_index[\"\\t\"] # Sequence always start with start sign \\t, so current character at the beginning is \\t\n",
    "    total_seq = current_char\n",
    "    \n",
    "    # Initialise beam\n",
    "    beam = [{'current_char': current_char, 'prob': 1.0, 'state1': state1, \n",
    "             'state2':state2,'total_seq':total_seq}]\n",
    "    \n",
    "    # Compute the 12-most likely output sequences (predictions for the past tense form)\n",
    "    for i in range(max_len):\n",
    "        candidates = []  # List to hold all candidate sequences for this step\n",
    "        \n",
    "        # Loop over each sequence in the current beam\n",
    "        for b in beam:\n",
    "            current_char = b['current_char']   # Get the current character (previously predicted one)\n",
    "            state1 = b['state1']  # Get the current state from the first encoder layer\n",
    "            state2 = b['state2']  # Get the current state from the second encoder layer\n",
    "            total_seq = b['total_seq']  # Get the total sequence so far\n",
    "            \n",
    "            # Predict the next character probabilities using the decoder model\n",
    "            probs, h1, c1, h2, c2 = dec_model.predict([current_char] + [encoded_seq] + state1 + state2, verbose=0)\n",
    "            \n",
    "            probs = probs[0][0]  # Get the probabilities for the next character\n",
    "            top_k = tf.argsort([probs], direction='DESCENDING')[0][:beam_width]  # Get the indices of the top-k probabilities\n",
    "            \n",
    "            # Generate new candidate sequences for the top-k characters\n",
    "            for k in top_k:\n",
    "                predicted_char = tf.expand_dims([k], 1)  # Create the predicted char tensor\n",
    "                total_seq = tf.concat([total_seq, predicted_char], axis=1)  # Append the predicted character to the total sequence\n",
    "                current_char = predicted_char  # Update current character\n",
    "                \n",
    "                candidate_prob = probs[k] * b['prob']  # Combine probabilities for total candidate probability\n",
    "                \n",
    "                # Update the states for the candidate sequence\n",
    "                candidate_state1 = [h1, c1]\n",
    "                candidate_state2 = [h2, c2]\n",
    "                \n",
    "                # Store the candidate sequence and its properties\n",
    "                candidates.append({'current_char': current_char, 'prob': candidate_prob, 'state1': candidate_state1, \n",
    "                                   'state2': candidate_state2, 'total_seq': total_seq})\n",
    "        \n",
    "        # Sort the candidates by probability\n",
    "        candidates.sort(key=lambda x: x['prob'], reverse=True)  \n",
    "        # Retain only the top K sequences for the next iteration\n",
    "        beam = candidates[:beam_width]  \n",
    "    \n",
    "    return beam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to compute the real verb accuracies and save errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(label, train_eid, decode_sequence, enc_model, dec_model, char_index, train_target_verbs, val_target_verbs, pairs, val_eid, max_len):\n",
    "    \n",
    "    # Dictionary to hold more than 1 correct answer for inflections (given that in Exp 1 and 2 'doubles' are included)\n",
    "    correct_answers = dict()\n",
    "    seen = []\n",
    "    \n",
    "    # Combine training and validation sets to find 'doubles'\n",
    "    all_eid = np.concatenate((train_eid, val_eid), axis=0)\n",
    "    all_target_verbs = train_target_verbs + val_target_verbs\n",
    "\n",
    "    # Fill correct_answers dictionary\n",
    "    for i, t in enumerate(all_eid):\n",
    "        t = all_eid[i:i + 1]\n",
    "        if pairs[i][0] in seen:\n",
    "            value = correct_answers[pairs[i][0]]\n",
    "            new_value = value + [all_target_verbs[i].strip()]\n",
    "            correct_answers[pairs[i][0]] = new_value\n",
    "        else:\n",
    "            seen.append(pairs[i][0])\n",
    "            answer = all_target_verbs[i].strip()\n",
    "            correct_answers[pairs[i][0]] = [answer]          \n",
    "    \n",
    "    # Compute training accuracy\n",
    "    print(\"computing training accuracy\")\n",
    "\n",
    "    correct_ones, errors = [], []\n",
    "    correct_train_regulars, correct_train_irregulars, error_train_regulars, error_train_irregulars = [], [], [], []\n",
    "\n",
    "    for i, inputs in enumerate(train_eid):\n",
    "        input_seq = train_eid[i:i + 1]\n",
    "        decoded_verb = decode_sequence(input_seq, enc_model, dec_model, max_len, char_index)\n",
    "\n",
    "        # Check if decoded verb is correct and save errors for analysis\n",
    "        if decoded_verb.strip() in correct_answers[pairs[i][0]]:\n",
    "            correct_ones.append([(pairs[i][0], pairs[i][1]), (decoded_verb.strip(), correct_answers[pairs[i][0]])]) # Present and past, decoded verb and correct answer\n",
    "            if label[i] == 'reg':\n",
    "                correct_train_regulars.append(pairs[i][1])\n",
    "            else:\n",
    "                correct_train_irregulars.append(pairs[i][1])\n",
    "        else:\n",
    "            errors.append([(pairs[i][0], pairs[i][1]), (decoded_verb.strip(), correct_answers[pairs[i][0]])]) # Present and past, decoded verb and correct answer\n",
    "            if label[i] == 'reg':\n",
    "                error_train_regulars.append([pairs[i][0], decoded_verb.strip(), correct_answers[pairs[i][0]]]) # Present, decoded verb and correct answer\n",
    "            else:\n",
    "                error_train_irregulars.append([pairs[i][0], decoded_verb.strip(), correct_answers[pairs[i][0]]]) # Present, decoded verb and correct answer\n",
    "\n",
    "    # Calculate training accuracies for overall, regular, and irregular verbs\n",
    "    train_accuracy = 100 * len(correct_ones) / (len(correct_ones) + len(errors))\n",
    "    irreg_train_acc = 100 * len(correct_train_irregulars) / (len(correct_train_irregulars) + len(error_train_irregulars))\n",
    "    reg_train_acc = 100 * len(correct_train_regulars) / (len(correct_train_regulars) + len(error_train_regulars))\n",
    "\n",
    "    # Compute validation accuracy\n",
    "    print(\"computing validation accuracy\")\n",
    "\n",
    "    val_correct_ones, val_errors = [], []\n",
    "    correct_val_regulars, correct_val_irregulars, error_val_regulars, error_val_irregulars = [], [], [], []\n",
    "\n",
    "    for i, inputs in enumerate(val_eid):\n",
    "        j = i + len(train_eid)  # index of the item in the correct_answer dictionary and 'pairs' and 'label' lists (i-th example of the validation set + length of training set)\n",
    "        input_seq = val_eid[i:i + 1]\n",
    "        decoded_verb = decode_sequence(input_seq, enc_model, dec_model, max_len, char_index)\n",
    "\n",
    "        # Check if decoded verb is correct and save both correct and incorrect inflections for analysis\n",
    "        if decoded_verb.strip() in correct_answers[pairs[j][0]]:\n",
    "            val_correct_ones.append([(pairs[j][0], pairs[j][1]), (decoded_verb.strip(), correct_answers[pairs[j][0]])]) # Present and past, decoded verb and correct answer\n",
    "            if label[j] == 'reg':\n",
    "                correct_val_regulars.append(pairs[j][1])\n",
    "            else:\n",
    "                correct_val_irregulars.append(pairs[j][1])\n",
    "        else:\n",
    "            val_errors.append([(pairs[j][0], pairs[j][1]), (decoded_verb.strip(), correct_answers[pairs[j][0]])]) # Present and past, decoded verb and correct answer\n",
    "            if label[j] == 'reg':\n",
    "                error_val_regulars.append([pairs[j][0], decoded_verb.strip(), correct_answers[pairs[j][0]]]) # Present, decoded verb and correct answer\n",
    "            else:\n",
    "                error_val_irregulars.append([pairs[j][0], decoded_verb.strip(), correct_answers[pairs[j][0]]])  # Present, decoded verb and correct answer\n",
    "\n",
    "    # Calculate validation accuracies for overall, regular, and irregular verbs\n",
    "    val_accuracy = 100 * len(val_correct_ones) / (len(val_correct_ones) + len(val_errors))\n",
    "    irreg_val_acc = 100 * len(correct_val_irregulars) / (len(correct_val_irregulars) + len(error_val_irregulars))\n",
    "    reg_val_acc = 100 * len(correct_val_regulars) / (len(correct_val_regulars) + len(error_val_regulars))   \n",
    "    \n",
    "    # Return all the computed accuracy and error details\n",
    "    train_a_i_r = [train_accuracy, irreg_train_acc, reg_train_acc]\n",
    "    val_a_i_r = [val_accuracy, irreg_val_acc, reg_val_acc]\n",
    "    \n",
    "    return train_a_i_r, val_a_i_r, error_train_regulars, error_train_irregulars, error_val_regulars, error_val_irregulars, correct_val_irregulars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute and print correlation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_correlations(results):\n",
    "    \n",
    "    BEAM_PROD_CORR,BEAM_RATING_CORR = [],[]\n",
    "    \n",
    "    # A&H experiment results (production experiment and rating experiment)\n",
    "    AH_df = pd.read_excel(\"AHresults.xlsx\")\n",
    "    AH_df_ratings = pd.read_excel(\"AHresults2.xlsx\")\n",
    "\n",
    "    # Structure model beam results in a similar way\n",
    "    beam_results = [[r[0][1],r[1][1]] for r in results]\n",
    "    beam_results_df = pd.DataFrame(beam_results, index=wugs, columns=['reg', 'irreg1'])\n",
    "    \n",
    "    # Regular wug inflection\n",
    "    x=beam_results_df[\"reg\"]\n",
    "    y=AH_df[\"reg\"]\n",
    "    # Irregular wug inflection\n",
    "    a=beam_results_df[\"irreg1\"]\n",
    "    b=AH_df[\"irreg1\"] \n",
    "\n",
    "    # Correlation betweem BEAM and production/rating probabilities A&H\n",
    "    print(\"Production probabilities vs. Beam probabilities\")\n",
    "    \n",
    "    # Compute and print correlations beam probs vs A&H production prob data, regular class\n",
    "    print(\"Regular class\")\n",
    "    corr, p_val = spearmanr(x, y)\n",
    "    BEAM_PROD_CORR.append([corr, p_val])\n",
    "    print(\"SPEARMAN:\")\n",
    "    print(f\"r = {corr:.4f} (p = {p_val:.4f})\")\n",
    "    corr, p_val = pearsonr(x, y)\n",
    "    BEAM_PROD_CORR.append([corr, p_val])\n",
    "    print(\"PEARSON:\")\n",
    "    print(f\"r = {corr:.4f} (p = {p_val:.4f})\")\n",
    "\n",
    "    # Compute and print correlations beam probs vs A&H production prob data, irregular class\n",
    "    corr, p_val = spearmanr(a, b)\n",
    "    BEAM_PROD_CORR.append([corr, p_val])\n",
    "    corr, p_val = pearsonr(a, b)\n",
    "    BEAM_PROD_CORR.append([corr, p_val])\n",
    "\n",
    "    # Beam probs vs A&H rating data\n",
    "    y2 = AH_df_ratings[\"reg\"]\n",
    "    b2 = AH_df_ratings[\"irreg1\"]\n",
    "    corr, p_val = spearmanr(x, y2)\n",
    "    BEAM_RATING_CORR.append([corr, p_val])\n",
    "    corr, p_val = pearsonr(x, y2)\n",
    "    BEAM_RATING_CORR.append([corr, p_val])\n",
    "    corr, p_val = spearmanr(a, b2)\n",
    "    BEAM_RATING_CORR.append([corr, p_val])\n",
    "    corr, p_val = pearsonr(a, b2)\n",
    "    BEAM_RATING_CORR.append([corr, p_val])\n",
    "\n",
    "    \n",
    "    # In the analysis notebook this function is extended to print and compute the correlations between\n",
    "        #the model's top 1 prediction as well. However, here, for individual runs of the model, it is not insightful.\n",
    "    \n",
    "    return BEAM_PROD_CORR,BEAM_RATING_CORR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN EXPERIMENT ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load real and nonce verb data\n",
    "pairs, written_pairs, freq, label, split = all_data(square_root_normalization, SEED, val_split, fold, excl_extra_verbs, remove_doubles)\n",
    "encoder_input_data, train_eid, train_target_verbs, decoder_input_data, decoder_target_data, val_eid, val_target_verbs, char_index, max_word_length, num_characters, val_data = create_model_data(split, pairs, freq, use_token_freq)\n",
    "wugs, wugs_reg, wugs_irreg1, wugs_irreg2, wug_input_data = all_wug_data(char_index, max_word_length)\n",
    "\n",
    "# Create the reversed character index for decoding\n",
    "reverse_char_index = dict((m, char) for char, m in char_index.items())\n",
    "\n",
    "# Create the 'other' category for the Wug Test\n",
    "wugs_other = [\"\"] * len(wugs)\n",
    "\n",
    "# Create list to save results per individual run/model\n",
    "individual_results = [[\"model nr\",\"fold\", \"num epochs\", \"train acc (All/Irreg/Reg)\", \"val_acc (All/Irreg/Reg)\",\n",
    "                       \"correlations\", \"wug beam results\", \"top 1 forms\", \"beam per wug\" ,                   \n",
    "                      \"reg train err\", \"irreg train err\", \"reg val err\",\"irreg val err\", \"irreg val CORRECT\"]]\n",
    "\n",
    "## EXPERIMENT ##\n",
    "\n",
    "for n in range(number_of_initialisations):\n",
    "    model_nr = n+1\n",
    "    print(\"Training model number: \", model_nr)\n",
    "    \n",
    "    # Train and build the encoder-decoder model\n",
    "    enc_model, dec_model, n_epochs = train_and_build_model(num_characters, max_word_length, encoder_input_data, decoder_input_data, decoder_target_data, val_data)\n",
    "        \n",
    "    # Evaluate model performance on training and validation set\n",
    "    print(\"Testing real verb performance\")\n",
    "    train_acc, val_acc, train_reg_err, train_irreg_err, val_reg_err, val_irreg_err, val_irreg_COR = compute_acc(label, train_eid, decode_sequence, enc_model, dec_model, char_index, train_target_verbs, val_target_verbs, pairs, val_eid, max_word_length)\n",
    "    \n",
    "    # Do Wug Test\n",
    "    print(\"Wug Test / nonce verb predictions\")\n",
    "    \n",
    "    # For each run compute Wug Test production probabilities\n",
    "    top1_forms, wug_result, beam_per_wug =[],[],[]\n",
    "    wug_result.append('model '+str(n+1))\n",
    "    for wug_reg, wug_irreg1, wug_irreg2, wug_other in zip(wugs_reg, wugs_irreg1, wugs_irreg2, wugs_other):\n",
    "        count = [[wug_reg,0.0],[wug_irreg1,0.0],[wug_irreg2,0.0],[wug_other,0.0]]\n",
    "        wug_result.append(count)\n",
    "    \n",
    "    # Save for each nonce verb in Wug Test results\n",
    "    for i,w in enumerate(wug_input_data): \n",
    "        # Beam search per wug\n",
    "        beam = beam_search(wug_input_data[i:i+1], enc_model, dec_model, beam_width=beam_width, max_len=max_word_length, char_index=char_index) \n",
    "        total_sum = 0\n",
    "\n",
    "        #For each of the 12 beam predictions decode the outputs to characters\n",
    "        for ib,b in enumerate(beam):\n",
    "            total_sum+=b['prob']\n",
    "            sequence = b['total_seq'].numpy().tolist()[0]\n",
    "            decoded_verb = \"\"\n",
    "            for char in sequence:\n",
    "                decoded_verb += reverse_char_index[char]\n",
    "            beam_per_wug.append([i, ib, decoded_verb.strip(), b['prob']])\n",
    "            \n",
    "            # Sort the beam predictions and probabilities into categories: regular/irregular1/irregular2/other\n",
    "            if decoded_verb.strip() == wugs_reg[i]:\n",
    "                wug_result[i+1][0][1] += b['prob'] # E.g. if beam prediction = regular inflection, reg += current beam probability \n",
    "                if ib==0: # Also save top 1 from beam\n",
    "                    top1_forms.append([decoded_verb.strip(),'reg',b['prob']]) \n",
    "            elif decoded_verb.strip() == wugs_irreg1[i]:\n",
    "                wug_result[i+1][1][1] += b['prob']\n",
    "                if ib==0:\n",
    "                    top1_forms.append([decoded_verb.strip(),'irreg1',b['prob']])\n",
    "            elif decoded_verb.strip() == wugs_irreg2[i]:\n",
    "                wug_result[i+1][2][1] += b['prob']\n",
    "                if ib==0:\n",
    "                    top1_forms.append([decoded_verb.strip(),'irreg2',b['prob']])\n",
    "            else:\n",
    "                wug_result[i+1][3][1] += b['prob']\n",
    "                if ib==0:\n",
    "                    top1_forms.append([decoded_verb.strip(),'other', b['prob']])\n",
    "\n",
    "    # Normalising beam probabilities per wug\n",
    "    for n,w in enumerate(wug_result[1:]):\n",
    "        summ=w[0][1]+w[1][1]+w[2][1]+w[3][1]\n",
    "        w[0][1]/=summ\n",
    "        w[1][1]/=summ\n",
    "        w[2][1]/=summ\n",
    "        w[3][1]/=summ\n",
    "\n",
    "    # Compute correlation coefficients based on wug predictions of current run\n",
    "    all_correlations = print_correlations(wug_result[1:])\n",
    "\n",
    "    # Saving the results of the current model\n",
    "    individual_result = [model_nr, fold, n_epochs, train_acc, val_acc, all_correlations, wug_result[1:], \n",
    "                         top1_forms, beam_per_wug, train_reg_err, train_irreg_err,\n",
    "                         val_reg_err, val_irreg_err, val_irreg_COR]\n",
    "    \n",
    "    individual_results.append(individual_result) \n",
    "\n",
    "    # Update fold after 5 runs and re-load data, depending on the experiment \n",
    "    if model_nr==5 and not exp1: \n",
    "        fold = 'second20'\n",
    "        pairs, written_pairs, freq, label, split = all_data(square_root_normalization, SEED, val_split, fold, excl_extra_verbs, remove_doubles)\n",
    "        encoder_input_data, train_eid, train_target_verbs, decoder_input_data, decoder_target_data, val_eid, val_target_verbs, char_index, max_word_length, num_characters, val_data = create_model_data(split, pairs, freq, use_token_freq)\n",
    "    elif model_nr==10:\n",
    "        fold = 'third20'\n",
    "        pairs, written_pairs, freq, label, split = all_data(square_root_normalization, SEED, val_split, fold, excl_extra_verbs, remove_doubles)\n",
    "        encoder_input_data, train_eid, train_target_verbs, decoder_input_data, decoder_target_data, val_eid, val_target_verbs, char_index, max_word_length, num_characters, val_data = create_model_data(split, pairs, freq, use_token_freq)\n",
    "    elif model_nr==15:\n",
    "        fold = 'fourth20'\n",
    "        pairs, written_pairs, freq, label, split = all_data(square_root_normalization, SEED, val_split, fold, excl_extra_verbs, remove_doubles)\n",
    "        encoder_input_data, train_eid, train_target_verbs, decoder_input_data, decoder_target_data, val_eid, val_target_verbs, char_index, max_word_length, num_characters, val_data = create_model_data(split, pairs, freq, use_token_freq)\n",
    "    elif model_nr==20 and exp_1_2:\n",
    "        fold='DEFAULT'\n",
    "        pairs, written_pairs, freq, label, split = all_data(square_root_normalization, SEED, val_split, fold, excl_extra_verbs, remove_doubles)\n",
    "        encoder_input_data, train_eid, train_target_verbs, decoder_input_data, decoder_target_data, val_eid, val_target_verbs, char_index, max_word_length, num_characters, val_data = create_model_data(split, pairs, freq, use_token_freq)\n",
    "\n",
    "\n",
    "    \n",
    "    #clear session for memory efficiency\n",
    "    tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(individual_results)\n",
    "df.columns = df.iloc[0]\n",
    "df = df[1:]\n",
    "df = pd.DataFrame(individual_results)\n",
    "df.to_excel(\"sgt.xlsx\",sheet_name='Sheet_name_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
