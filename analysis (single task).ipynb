{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "from scipy.stats import pearsonr,spearmanr\n",
    "from ast import literal_eval\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that:\n",
    "- \"split 1\" in the thesis report refers \"fold 5\"/\"DEFAULT\" here. This datasplit used for Experiment 1.\n",
    "- \"val\"/\"validation\" here means \"development set\" that is mentioned in the thesis report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading experiment output\n",
    "all_results = pd.read_excel('sgt.xlsx')\n",
    "all_results = all_results[1:]\n",
    "\n",
    "# Save to list results Wug Test data\n",
    "ALL_BEAMS = [literal_eval(beam) for beam in all_results[6]] # Using literal_eval() to read Wug Test beam predictions and top 1 predictions\n",
    "ALL_TOP1S = [[wugs[0], wugs[1]] for top1_preds in all_results[7] for wugs in literal_eval(top1_preds)]\n",
    "\n",
    "# Lists contain for each of the 25 models:\n",
    "# ALL_BEAMS = production probabiltiy for each nonce verb based on the beam predictions, e.g. : 'bize' -> 'bized' 0.85, 'boze' 0.00, other 0.15\n",
    "# ALL_TOP1S = the top 1 prediction for each wug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Wug Test nonce verbs \n",
    "\n",
    "In the same way as in the experiment notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_wug_data():\n",
    "    wugs,wugs_reg,wugs_irreg1,wugs_irreg2=[],[],[],[]\n",
    "    \n",
    "    # Load all wug forms\n",
    "    with open(\"KCamericanwugs.txt\", encoding=\"utf8\") as KCwug_data: # A&H nonce verb present tense in K&C AE phonetic transcriptions\n",
    "        for wug in KCwug_data:\n",
    "            wugs.append(\"\".join(wug.split()))\n",
    "            \n",
    "    with open(\"reg.txt\", encoding=\"utf8\") as KCwug_data: # A&H regular inflection in K&C AE phonetic transcriptions\n",
    "        for wug in KCwug_data:\n",
    "            wugs_reg.append(\"\".join(wug.split()))       \n",
    "            \n",
    "    with open(\"irreg1.txt\", encoding=\"utf8\") as KCwug_data: # A&H irregular inflection in K&C AE phonetic transcriptions\n",
    "        for wug in KCwug_data:\n",
    "            wugs_irreg1.append(\"\".join(wug.split()))    \n",
    "            \n",
    "    with open(\"irreg2.txt\", encoding=\"utf8\") as KCwug_data: # A&H 2nd option irregular inflection in K&C AE phonetic transcriptions\n",
    "        for wug in KCwug_data:\n",
    "            wugs_irreg2.append(\"\".join(wug.split()))\n",
    "    return wugs, wugs_reg ,wugs_irreg1, wugs_irreg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wugs, wugs_reg ,wugs_irreg1, wugs_irreg2 = all_wug_data()\n",
    "\n",
    "# Prepare to aggregate the beam predictions/probabilities: create a clear list\n",
    "\n",
    "wugs_other=[]\n",
    "for i in range(len(wugs)):\n",
    "    wugs_other.append(\"\")\n",
    "\n",
    "ALL_results=[]\n",
    "for wug_reg, wug_irreg1, wug_irreg2, wug_other in zip(wugs_reg, wugs_irreg1, wugs_irreg2, wugs_other):\n",
    "    count = [[wug_reg,0.0], [wug_irreg1,0.0], [wug_irreg2,0.0], [wug_other,0.0]]\n",
    "    ALL_results.append(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to compute all correlations\n",
    "\n",
    "In the same way as in the experiment notebook -- except here is the extended version taking into account A&H rating data and top 1 predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_correlations(results,top1_forms):\n",
    "    \n",
    "    BEAM_PROD_CORR,BEAM_RATING_CORR,TOP1_PROD_CORR,TOP1_RATING_CORR = [],[],[],[]\n",
    "    \n",
    "    # Open A&H experiment results (production experiment and rating experiment)\n",
    "    AH_df = pd.read_excel(\"AHresults.xlsx\")\n",
    "    AH_df_ratings = pd.read_excel(\"AHresults2.xlsx\")\n",
    "\n",
    "    # Structure model beam results in a similar way\n",
    "    beam_results = [[r[0][1],r[1][1]] for r in results]\n",
    "    beam_results_df = pd.DataFrame(beam_results, index=wugs, columns=['reg', 'irreg1'])\n",
    "    \n",
    "    # Regular wug inflection\n",
    "    x=beam_results_df[\"reg\"]\n",
    "    y=AH_df[\"reg\"]\n",
    "    # Irregular wug inflection\n",
    "    a=beam_results_df[\"irreg1\"]\n",
    "    b=AH_df[\"irreg1\"] \n",
    "\n",
    "    # Correlation betweem BEAM and production/rating probabilities A&H\n",
    "    print(\"Production probabilities vs. Beam probabilities\\n\")\n",
    "    \n",
    "    # Regular class\n",
    "    print(\"Regular class\")\n",
    "    corr, p_val = spearmanr(x, y)\n",
    "    BEAM_PROD_CORR.append([corr, p_val])\n",
    "    print(\"SPEARMAN:\")\n",
    "    print(f\"r = {corr:.4f} (p = {p_val:.4f})\")\n",
    "    \n",
    "    corr, p_val = pearsonr(x, y)\n",
    "    BEAM_PROD_CORR.append([corr, p_val])\n",
    "    print(\"PEARSON:\")\n",
    "    print(f\"r = {corr:.4f} (p = {p_val:.4f})\")\n",
    "\n",
    "    # Irregular 1 class\n",
    "    print(\"Irregular 1 class\")\n",
    "    corr, p_val = spearmanr(a, b)\n",
    "    BEAM_PROD_CORR.append([corr, p_val])\n",
    "    print(\"SPEARMAN:\")\n",
    "    print(f\"r = {corr:.4f} (p = {p_val:.4f})\")\n",
    "    \n",
    "    corr, p_val = pearsonr(a, b)\n",
    "    BEAM_PROD_CORR.append([corr, p_val])\n",
    "    print(\"PEARSON:\")\n",
    "    print(f\"r = {corr:.4f} (p = {p_val:.4f})\")\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    # Ratings vs. Beam probabilities\n",
    "    print(\"Ratings vs. Beam probabilities\")\n",
    "    y2 = AH_df_ratings[\"reg\"]\n",
    "    b2 = AH_df_ratings[\"irreg1\"]\n",
    "\n",
    "    # Regular class\n",
    "    print(\"Regular class\")\n",
    "    corr, p_val = spearmanr(x, y2)\n",
    "    BEAM_RATING_CORR.append([corr, p_val])\n",
    "    print(\"SPEARMAN:\")\n",
    "    print(f\"r = {corr:.4f} (p = {p_val:.4f})\")\n",
    "    \n",
    "    corr, p_val = pearsonr(x, y2)\n",
    "    BEAM_RATING_CORR.append([corr, p_val])\n",
    "    print(\"PEARSON:\")\n",
    "    print(f\"r = {corr:.4f} (p = {p_val:.4f})\")\n",
    "\n",
    "    # Irregular 1 class\n",
    "    print(\"Irregular 1 class\")\n",
    "    corr, p_val = spearmanr(a, b2)\n",
    "    BEAM_RATING_CORR.append([corr, p_val])\n",
    "    print(\"SPEARMAN:\")\n",
    "    print(f\"r = {corr:.4f} (p = {p_val:.4f})\")\n",
    "    \n",
    "    corr, p_val = pearsonr(a, b2)\n",
    "    BEAM_RATING_CORR.append([corr, p_val])\n",
    "    print(\"PEARSON:\")\n",
    "    print(f\"r = {corr:.4f} (p = {p_val:.4f})\")\n",
    "    \n",
    "\n",
    "    # Correlation betweem top 1 prediction and production/rating probabilities A&H\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"Production probabilities vs. Top 1 Production Probabilities\\n\")\n",
    "    \n",
    "    top1_count = [[[wug_reg,0],[wug_irreg1,0]] for wug_reg,wug_irreg1 in zip(wugs_reg,wugs_irreg1)]\n",
    "    nn=0\n",
    "   \n",
    "    for form in top1_forms:\n",
    "        if nn % 58 == 0: # 58 is the number of nonce verb per wug test, so after every 58 nonce verbs, reset because it's a model/test\n",
    "            nn=0 #nn is the current wug verb number\n",
    "        if form[0]==top1_count[nn][0][0]: #count for wug verb nn how often it is regularly inflected\n",
    "            top1_count[nn][0][1]+=1\n",
    "        elif form[0]==top1_count[nn][1][0]:  #count for wug verb nn how often it is irregularly inflected\n",
    "            top1_count[nn][1][1]+=1\n",
    "        nn+=1\n",
    "\n",
    "    division=len(top1_forms)/58  # 58 is the number of nonce verb per wug test, so this results in the number of experiments (i.e. in my case 25)\n",
    "    for c in top1_count:\n",
    "        c[0][1]/=division\n",
    "        c[1][1]/=division\n",
    "\n",
    "    top1_results = [[r[0][1],r[1][1]] for r in top1_count]\n",
    "    top1_results_df = pd.DataFrame(top1_results, index=wugs, columns=['reg', 'irreg1'])  \n",
    "    x3=top1_results_df[\"reg\"]\n",
    "    a3=top1_results_df[\"irreg1\"]\n",
    "        \n",
    "    # Regular class\n",
    "    print(\"Regular class\")\n",
    "    corr, p_val = spearmanr(x3, y)\n",
    "    TOP1_PROD_CORR.append([corr, p_val])\n",
    "    print(\"SPEARMAN:\")\n",
    "    print(f\"r = {corr:.4f} (p = {p_val:.4f})\")\n",
    "    \n",
    "    corr, p_val = pearsonr(x3, y)\n",
    "    TOP1_PROD_CORR.append([corr, p_val])\n",
    "    print(\"PEARSON:\")\n",
    "    print(f\"r = {corr:.4f} (p = {p_val:.4f})\")\n",
    "\n",
    "    # Irregular 1 class\n",
    "    print(\"Irregular 1 class\")\n",
    "    corr, p_val = spearmanr(a3, b)\n",
    "    TOP1_PROD_CORR.append([corr, p_val])\n",
    "    print(\"SPEARMAN:\")\n",
    "    print(f\"r = {corr:.4f} (p = {p_val:.4f})\")\n",
    "    \n",
    "    corr, p_val = pearsonr(a3, b)\n",
    "    TOP1_PROD_CORR.append([corr, p_val])\n",
    "    print(\"PEARSON:\")\n",
    "    print(f\"r = {corr:.4f} (p = {p_val:.4f})\")\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    # Ratings vs. Top 1 probabilities\n",
    "    print(\"Ratings vs. Top 1 probabilities\")\n",
    "    y2 = AH_df_ratings[\"reg\"]\n",
    "    b2 = AH_df_ratings[\"irreg1\"]\n",
    "\n",
    "    # Regular class\n",
    "    print(\"Regular class\")\n",
    "    corr, p_val = spearmanr(x3, y2)\n",
    "    TOP1_RATING_CORR.append([corr, p_val])\n",
    "    print(\"SPEARMAN:\")\n",
    "    print(f\"r = {corr:.4f} (p = {p_val:.4f})\")\n",
    "    \n",
    "    corr, p_val = pearsonr(x3, y2)\n",
    "    TOP1_RATING_CORR.append([corr, p_val])\n",
    "    print(\"PEARSON:\")\n",
    "    print(f\"r = {corr:.4f} (p = {p_val:.4f})\")\n",
    "\n",
    "    # Irregular 1 class\n",
    "    print(\"Irregular 1 class\")\n",
    "    corr, p_val = spearmanr(a3, b2)\n",
    "    TOP1_RATING_CORR.append([corr, p_val])\n",
    "    print(\"SPEARMAN:\")\n",
    "    print(f\"r = {corr:.4f} (p = {p_val:.4f})\")\n",
    "    \n",
    "    corr, p_val = pearsonr(a3, b2)\n",
    "    TOP1_RATING_CORR.append([corr, p_val])\n",
    "    print(\"PEARSON:\")\n",
    "    print(f\"r = {corr:.4f} (p = {p_val:.4f})\")\n",
    "\n",
    "    return BEAM_PROD_CORR,BEAM_RATING_CORR,TOP1_PROD_CORR,TOP1_RATING_CORR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALYSIS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGGREGATE CORRELATION (n=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate all (n=25) beam production probabilities\n",
    "for model in ALL_BEAMS:\n",
    "    model = np.array(model)\n",
    "    for i,wug in enumerate(model):\n",
    "        for j,form in enumerate(wug):\n",
    "            ALL_results[i][j][1]+=float(form[1])\n",
    "\n",
    "\n",
    "# Normalize the probabilties, dividing by 25 because there are 25 models\n",
    "for results in ALL_results:\n",
    "    results[0][1]/=25\n",
    "    results[1][1]/=25\n",
    "    results[2][1]/=25\n",
    "    results[3][1]/=25 \n",
    "\n",
    "all_correlations = print_correlations(ALL_results,ALL_TOP1S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGGREGATE CORRELATION PER FOLD (5 * n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For each of the five folds\n",
    "for i in range(5):\n",
    "    print(\"\\n\\n\\nCORRELATION FOLD \", i+1, '\\n')\n",
    "    \n",
    "    # Make a list to aggregate all inflection predictions\n",
    "    fold_results=[]\n",
    "    for wug_reg, wug_irreg1, wug_irreg2,wug_other in zip(wugs_reg, wugs_irreg1, wugs_irreg2, wugs_other):\n",
    "        count = [[wug_reg, 0.0],[wug_irreg1, 0.0],[wug_irreg2, 0.0],[wug_other, 0.0]]\n",
    "        fold_results.append(count)\n",
    "\n",
    "    # Aggregate and save the predictions from the 5 runs of the current fold\n",
    "    where, towhere  = i*5, (i+1)*5 # for each fold take the next five runs \n",
    "    for model in ALL_BEAMS[where:towhere]:\n",
    "        model = np.array(model)\n",
    "        for i, wug in enumerate(model):\n",
    "            for j, form in enumerate(wug):\n",
    "                fold_results[i][j][1] += float(form[1])\n",
    "                \n",
    "    fold_top1 = ALL_TOP1S[where*58:towhere*58] # Top 1 forms are all stored in one list, so just take the five chunks of 58 nonce verbs of the current fold\n",
    "    fold_correlations = print_correlations(fold_results, fold_top1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AVERAGE CORRELATION (n=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute correlation for each individual model\n",
    "\n",
    "Ignore the print below the next cell, it prints the individual correlations, not the averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_individual_correlations=[]\n",
    "print(\"\\n IGNORE THE PRINT BELOW! (these are not the averages, but individual correlations)\\n\\n\\n\\n\\n\")\n",
    "for i, individual_beam in enumerate(ALL_BEAMS): # Loop over the 25 simulations\n",
    "    individual_top1 = ALL_TOP1S[i*58:(i+1)*58] # There are 58 top 1 predictions per simulation\n",
    "    individual_correlation = print_correlations(individual_beam, individual_top1) \n",
    "    all_individual_correlations.append(individual_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the average of the computed correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_prod,beam_rate = [0]*4, [0]*4\n",
    "beam_prod0,beam_prod1,beam_prod2,beam_prod3=[],[],[],[]\n",
    "beam_rate0,beam_rate1,beam_rate2,beam_rate3=[],[],[],[]\n",
    "\n",
    "# Sort the individual correlations for the beam predictions into separate lists (regular and irregular class; spearman and pearson; A&H production and rating data)\n",
    "for X in all_individual_correlations:\n",
    "    #beam probs on production\n",
    "    beam_prod0.append(X[0][0][0])   #reg spearman\n",
    "    beam_prod1.append(X[0][1][0])   #reg pearson\n",
    "    beam_prod2.append(X[0][2][0])   #irreg spearman\n",
    "    beam_prod3.append(X[0][3][0])   #irreg pearson\n",
    "    #beam probs on rating\n",
    "    beam_rate0.append(X[1][0][0])   #reg spearman\n",
    "    beam_rate1.append(X[1][1][0])   #reg pearson\n",
    "    beam_rate2.append(X[1][2][0])   #irreg spearman\n",
    "    beam_rate3.append(X[1][3][0])   #irreg pearson\n",
    "    \n",
    "beam_prod = [beam_prod0,beam_prod1,beam_prod2,beam_prod3]\n",
    "beam_rate = [beam_rate0,beam_rate1,beam_rate2,beam_rate3]\n",
    "\n",
    "# Compute the average correlations \n",
    "avg_bps,avg_brs=[],[],\n",
    "for bp in beam_prod:\n",
    "    avg_bps.append(round(statistics.mean(bp), 2))  \n",
    "for br  in beam_rate:\n",
    "    avg_brs.append(round(statistics.mean(br), 2))  \n",
    "\n",
    "print(\"beam probs vs A&H production probs:\")    \n",
    "print(avg_bps)\n",
    "print(\"beam probs vs A&H rating:\")\n",
    "print(avg_brs)\n",
    "\n",
    "print(\"\\n\\nstructured as: avg reg spearman, avg reg pearson, avg irreg spearman, avg irreg pearson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AVERAGE ACCURACY (n=25) REAL VERBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_T, irreg_T, reg_T, overall_V, irreg_V, reg_V = [], [], [], [], [], []\n",
    "\n",
    "# Aggregate the 25 individual accuracy on the real verbs (training and development)\n",
    "for x, y in zip(all_results[3], all_results[4]):\n",
    "    X = x.strip('][').split(', ')\n",
    "    Y = y.strip('][').split(', ')\n",
    "    overall_T.append(float(X[0]))\n",
    "    irreg_T.append(float(X[1]))\n",
    "    reg_T.append(float(X[2]))\n",
    "    overall_V.append(float(Y[0]))\n",
    "    irreg_V.append(float(Y[1]))\n",
    "    reg_V.append(float(Y[2]))\n",
    "\n",
    "categories = [(\"Training Overall\", overall_T), (\"Training Irregular\", irreg_T), (\"Training Regular\", reg_T),\n",
    "              (\"Validation Overall\", overall_V), (\"Validation Irregular\", irreg_V), (\"Validation Regular\", reg_V)]\n",
    "\n",
    "# For each category, print the average accuracy and standard deviation\n",
    "for label, data in categories:\n",
    "    print(f\"{label}\\t {statistics.mean(data):.2f}% \\t (SD = {statistics.stdev(data):.2f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per fold (5 * n=5)\n",
    "\n",
    "REMINDER: in the report 'split 1' is actually 'Fold 5' here. This is the data split used for Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same code as above, but now looping over the 5 folds to compute the average over 5 runs each time\n",
    "\n",
    "b, e = 0, 5\n",
    "\n",
    "# For each of the five folds\n",
    "for f in range(5):\n",
    "    \n",
    "    print(\"\\nFold\", f + 1)\n",
    "    \n",
    "    overall_T, irreg_T, reg_T, overall_V, irreg_V, reg_V = [], [], [], [], [], []\n",
    "    \n",
    "    # Append the accuracies of the 5 runs of the current fold, per class: overall, iregular, regular\n",
    "    for x, y in zip(all_results[3][b:e], all_results[4][b:e]):\n",
    "        X = x.strip('][').split(', ')\n",
    "        Y = y.strip('][').split(', ')\n",
    "        \n",
    "        overall_T.append(float(X[0]))\n",
    "        irreg_T.append(float(X[1]))\n",
    "        reg_T.append(float(X[2]))\n",
    "        overall_V.append(float(Y[0]))\n",
    "        irreg_V.append(float(Y[1]))\n",
    "        reg_V.append(float(Y[2]))\n",
    "            \n",
    "    categories = [(\"Training Overall\", overall_T),(\"Training Irregular\", irreg_T),(\"Training Regular\", reg_T),\n",
    "                  (\"Validation Overall\", overall_V), (\"Validation Irregular\", irreg_V), (\"Validation Regular\", reg_V)]\n",
    "    \n",
    "    # Compute average and standard deviation for current fold\n",
    "    for label, data in categories:\n",
    "        print(f\"{label}\\t {statistics.mean(data):.2f}% \\t (SD = {statistics.stdev(data):.2f})\")\n",
    "\n",
    "    # Update b and e for the next fold\n",
    "    b += 5\n",
    "    e += 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AVERAGE NUMBER OF TRAINING EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to calculate and print average epochs for a given range\n",
    "def print_average_epochs(results, start, end, fold_name):\n",
    "    epochs = sum(int(e) for e in results[start:end])\n",
    "    print(f\"{fold_name}\")\n",
    "    print(f\"Average epochs = {epochs / (end - start):.2f}\")\n",
    "\n",
    "# Number of epochs data from results\n",
    "epochs_data = all_results[2]\n",
    "\n",
    "# Global average number of epochs\n",
    "print(\"Global average epochs:\")\n",
    "print(f\"{sum(int(e) for e in epochs_data) / 25:.2f}\")\n",
    "print(\" \")\n",
    "\n",
    "# Average number of epochs per fold \n",
    "fold_sizes = [5, 5, 5, 5, len(epochs_data) - 21]  # Last fold takes the remaining data\n",
    "start_index = 1\n",
    "\n",
    "for i, fold_size in enumerate(fold_sizes):\n",
    "    end_index = start_index + fold_size\n",
    "    fold_name = f\"Fold {i + 1}\"\n",
    "    print_average_epochs(epochs_data, start_index, end_index, fold_name)\n",
    "    start_index = end_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERROR CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_irreg_train = all_results[10]\n",
    "exp_irreg_val = all_results[12]\n",
    "exp_reg_train = all_results[9]\n",
    "exp_reg_val = all_results[11]\n",
    "all_train_irreg_errors = [literal_eval(x) for x in exp_irreg_train]\n",
    "all_train_reg_errors = [literal_eval(x) for x in exp_reg_train]\n",
    "all_val_irreg_errors = [literal_eval(x) for x in exp_irreg_val]\n",
    "all_val_reg_errors = [literal_eval(x) for x in exp_reg_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_errors(irreg_errors, reg_errors):\n",
    "    total_num_errors = 0\n",
    "    num_overregs = 0\n",
    "    num_blendings = 0\n",
    "    \n",
    "    obvious_overregs, check_overregs, blendings, copy, OTHER = [], [], [], [], []\n",
    "    \n",
    "    for model in reg_errors:\n",
    "        total_num_errors+=len(model)\n",
    "\n",
    "    # Loop through the aggregate set of incorrectly inflected verbs\n",
    "    for model in irreg_errors:\n",
    "        total_num_errors += len(model)\n",
    "        for error in model:\n",
    "            present = error[0]\n",
    "            inflection = error[1]\n",
    "            correct_past = error[2][0]\n",
    "        \n",
    "            # Regular verbs\n",
    "            if correct_past == present+ \"d\" or correct_past == present + \"«d\" or correct_past == present + \"t\" or correct_past == \"È\" + present + 't':\n",
    "                OTHER.append(error)\n",
    "                \n",
    "            # If the incorrect inflection of the verb is ...\n",
    "            \n",
    "            # the present form, it is a COPY ERROR\n",
    "            elif inflection == present:\n",
    "                copy.append(error)\n",
    "                OTHER.append(error) #i.e., other includes copy errors, because I ended up not using the copy class!\n",
    "            # the present form + d/ed, it is OVERREGULARISATION\n",
    "            elif inflection == present + \"d\" or inflection == present + \"«d\":\n",
    "                obvious_overregs.append(error)\n",
    "            # the present form + t but the correct inflection is present form + d/ed, it is NOT overregularisation\n",
    "            elif correct_past[:-1] == inflection[:-1] and correct_past[-1] == 'd' and inflection[-1] == 't': # e.g. \"thaw - thawt\" (instead of thawed) and \"beg - begt\" (instead of begged)\n",
    "                OTHER.append(error)\n",
    "            # the other present forms + t are OVERREGULARISATIONs (but thrown in a separate list so that I could double check manually)\n",
    "            elif inflection == present + \"t\":\n",
    "                check_overregs.append(error)\n",
    "            # the correct past tense form, but ALSO inflected this irregularly, it is a BLENDING ERROR\n",
    "            elif inflection == correct_past + \"d\" or inflection == correct_past + \"«d\" or inflection == correct_past + \"t\":\n",
    "                blendings.append(error)\n",
    "            else:\n",
    "                OTHER.append(error)\n",
    "    \n",
    "    # Over-regularization and blending statistics\n",
    "    num_overregs = len(obvious_overregs) + len(check_overregs)\n",
    "    num_blendings = len(blendings)\n",
    "\n",
    "    print(\"Total number of errors (aggregated):\", total_num_errors)\n",
    "    print(\"Total number of over-regularization errors:\", num_overregs)\n",
    "    print(\"Total number of blending errors:\", num_blendings)\n",
    "\n",
    "    return obvious_overregs, check_overregs, blendings, copy, OTHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data analysis:\")\n",
    "obvious_overregs, check_overregs, blendings, copy, OTHER = analyze_errors(all_train_irreg_errors, all_train_reg_errors)\n",
    "\n",
    "print(\"\\nValidation data analysis:\")\n",
    "val_obvious_overregs, val_check_overregs, val_blendings, val_copy, val_OTHER = analyze_errors(all_val_irreg_errors, all_val_reg_errors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular inflection overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_form_prob = 0\n",
    "\n",
    "# Compute the average probabilty for inflecting nonce verbs regularly based on the aggregate beam results (n=25)\n",
    "for beams in ALL_results:\n",
    "    regular_form_prob += beams[0][1]    \n",
    "print(\"Overall production probability for the regular form: \", round(100*regular_form_prob/58,2))\n",
    "\n",
    "# Compute how often the regular class was the largest probability (i.e. larger than the other inflection classes irreg 1,2 and 'other')\n",
    "regular_form_beam = 0\n",
    "for beams in ALL_results:\n",
    "    if beams[0][1] >= beams[1][1] and beams[0][1] >= beams[2][1] and beams[0][1] >= beams[3][1]:\n",
    "        regular_form_beam += 1\n",
    "print(\"Percentage regular inflection highest beam prob (aggregated n=25): \", round(100*regular_form_beam/58,2), \"(\",regular_form_beam,\" out of 58 wugs).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALIZATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I save excel files, so that i can plot graphs in excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wugs_ortho.txt\") as wugs_ortho:\n",
    "    ortho_wugs = [wug.split(\"\\t\") for wug in wugs_ortho][1:]\n",
    "pres_ortho_wugs = [wug[0] for wug in ortho_wugs]\n",
    "\n",
    "regular,irregular1,irregular2,other=[],[],[],[]\n",
    "\n",
    "f=[]\n",
    "for i,F in enumerate(ALL_results):\n",
    "    if F[2][0]!=\"\":\n",
    "        f.append(F[2][1])\n",
    "    else:\n",
    "        f.append(float(0))\n",
    "df_results=[]\n",
    "for i,w in enumerate(ALL_results):\n",
    "    df_results.append([pres_ortho_wugs[i],100*w[0][1],100*w[1][1],100*f[i],100*w[3][1]])\n",
    "    regular.append(100*w[0][1])\n",
    "    irregular1.append(100*w[1][1])\n",
    "    irregular2.append(100*f[i])#[2])    \n",
    "    other.append(100*w[3][1])\n",
    "df_results = pd.DataFrame(df_results, columns=['wug', 'regular', 'irregular 1', 'irregular 2', 'other'])\n",
    "\n",
    "df_results.to_excel(\"plot.xlsx\") \n",
    "\n",
    "# I plot the results in excel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
